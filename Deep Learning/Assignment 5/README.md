# Deep Learning – Assignment 5: Transformer-based Language Modeling

## Table of Contents
- [Deep Learning – Assignment 5: Transformer-based Language Modeling](#deep-learning--assignment-5-transformer-based-language-modeling)
  - [Table of Contents](#table-of-contents)
  - [Overview](#overview)
  - [Objectives](#objectives)
  - [Implementation Details](#implementation-details)
    - [Data Preprocessing](#data-preprocessing)
    - [Transformer Model Architecture](#transformer-model-architecture)
    - [Training Procedure](#training-procedure)
    - [Evaluation Metrics](#evaluation-metrics)
  - [Results \& Visualizations](#results--visualizations)

---

## Overview
In Assignment 5, we implement a Transformer-based language model to learn and generate text. This project covers all the steps from data preprocessing and tokenization, through building and training the Transformer model, to evaluating its performance using metrics like cross-entropy loss and perplexity, and generating sample text outputs.

---

## Objectives
- Build a Transformer model to capture long-range dependencies in text.
- Understand the mechanics of self-attention and positional encoding.
- Train the model on a text corpus for next-token prediction.
- Evaluate the model quantitatively and qualitatively through generated text.

---

## Implementation Details

### Data Preprocessing
- Load raw text data from a source file.
- Clean the text by converting to lowercase and removing punctuation.
- Tokenize the text into sequences of words.
- Create a vocabulary mapping tokens to integer indices.
- Pad sequences to a fixed length to form uniform batches.

### Transformer Model Architecture
- **Embedding Layer**: Converts token indices into dense vector representations.
- **Positional Encoding**: Adds positional information to token embeddings.
- **Transformer Blocks**: Consist of multi-head self-attention layers and feed-forward networks, along with layer normalization and dropout.
- **Output Layer**: A linear layer that projects the Transformer outputs into the vocabulary space, followed by a softmax to obtain probability distributions over the next token.

### Training Procedure
- Split the dataset into training, validation, and test sets.
- Use cross-entropy loss as the objective function.
- Train the model using an optimizer (e.g., Adam) with an appropriate learning rate.
- Monitor training and validation loss over multiple epochs.
- Optionally, employ learning rate scheduling and early stopping to improve convergence.

### Evaluation Metrics
- Evaluate the model using cross-entropy loss and perplexity.
- Generate sample sequences from the model to assess the quality of text generation.
- Analyze attention weight visualizations (if implemented) to understand the focus of the model.

---

## Results & Visualizations
1. **Loss and Perplexity Curves**  
   *Plots showing the training and validation loss and perplexity decreasing over epochs, indicating successful learning.*

2. **Sample Generated Text**  
   *Examples of text generated by the model, demonstrating its ability to capture coherent language structures.*

3. **Attention Visualizations (Optional)**  
   *Heatmaps of attention weights that reveal which parts of the input the model focuses on during prediction.*